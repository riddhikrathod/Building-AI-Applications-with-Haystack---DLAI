{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  \n",
    "- DeepSet is the creator of Haystack.  \n",
    "- Haystack is a popular open-source AI framework that allows developers to build various AI applications.  \n",
    "\n",
    "### Benefits of using a framework like Haystack  \n",
    "- Faster development  \n",
    "- Improves code maintainability and readability  \n",
    "- Manages complexity by enabling plug-and-play integration of different models and components  \n",
    "- Allows applications to be developed at a higher level of abstraction  \n",
    "- Makes it easy to switch underlying components of a workflow (such as vector databases) without extensive refactoring  \n",
    "- Provides common features out of the box (such as branching and looping)  \n",
    "- Provides visualization utilities  \n",
    "\n",
    "Haystack provides a common interface and simple abstractions that you can extend to fit your needs.  \n",
    "\n",
    "It is based on two main elements: **Components** and **Pipelines**.  \n",
    "\n",
    "- Components are connected to build powerful pipelines  \n",
    "- Both standard and custom components are supported  \n",
    "\n",
    "### What you will learn  \n",
    "- The unique building blocks that make up the Haystack framework  \n",
    "- How to use Haystack to build LLM applications  \n",
    "- Core abstractions of Haystack: components, pipelines, document stores, etc.  \n",
    "- How to build a customized RAG pipeline  \n",
    "- How to create custom components  \n",
    "- How to implement conditional routing and branching pipelines with a fallback to web search  \n",
    "- How to build a self-reflecting agent using Haystackâ€™s pipeline looping mechanism  \n",
    "- How to create a chat agent that uses OpenAI's function-calling capability, enabling Haystack pipelines to be used as tools  \n",
    "\n",
    "---\n",
    "\n",
    "# Haystack Building Blocks  \n",
    "\n",
    "- AI applications are often made up of multiple steps that work together to achieve a goal  \n",
    "- Each small task (step) in Haystack is handled by a **component**  \n",
    "- Components are combined with other components to form a **pipeline**\n",
    "- Pipelines are the entities that enable us to build the desired application  \n",
    "\n",
    "<center>\n",
    "  <img src=\"images/intro.png\"/>\n",
    "</center>  \n",
    "\n",
    "- A pipeline can access document stores (vector databases) through components  \n",
    "- A component can accept any number of inputs and produce any number of outputs  \n",
    "\n",
    "<center>\n",
    "  <img src=\"images/components.png\"/>\n",
    "</center>  \n",
    "\n",
    "Example: `SentenceTransformersDocumentEmbedder`  \n",
    "- Expects a list of documents  \n",
    "- Returns the same documents enriched with embeddings and metadata  \n",
    "- Uses the **sentence-transformers** embedding model  \n",
    "\n",
    "<center>\n",
    "  <img src=\"images/sentence_transformer.png\"/>\n",
    "</center>  \n",
    "\n",
    "Example of a Document Search Pipeline:\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/example_pipeline.png\"/>\n",
    "</center>  \n",
    "\n",
    "Haystack offers many ready-made components such as generators, embedders, retrievers, converters, rerankers, routers, and preprocessors. It also supports branching, decision-making components, custom components, looping, and advanced pipelines.  \n",
    "\n",
    "By combining ready-made and custom components, it is possible to create pipelines for tasks such as:  \n",
    "- Question answering  \n",
    "- Document search  \n",
    "- Chat  \n",
    "- Question generation  \n",
    "- Output validation  \n",
    "and more.  \n",
    "\n",
    "More on:\n",
    "- Components: <https://docs.haystack.deepset.ai/docs/components?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- Pipelines: <https://docs.haystack.deepset.ai/docs/pipelines?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- Document Stores: <https://docs.haystack.deepset.ai/docs/document-store?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- InMemoryDocumentStore: <https://docs.haystack.deepset.ai/docs/inmemorydocumentstore?utm_campaign=developer-relations&utm_source=dlai>\n",
    "\n",
    "#### Document Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from helper import load_env\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use the `OpenAIDocumentEmbedder` which uses OpenAI Embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-small\")\n",
    "embedder # inspect component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.dataclasses import Document\n",
    "documents = [Document(content=\"Haystack is an open source AI framework to build full AI applications in Python\"),\n",
    "             Document(content=\"You can build AI Pipelines by combining Components\"),]\n",
    "documents #inspect component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.run(documents=documents) # produces list of documents with embeddings along with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now create a pipeline by initializing a document store and then writing documents along side with their embeddings. For now, we will use the `InMemoryDocumentStore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our first indexing pipeline using the following components:\n",
    "1. converter `TextFileToDocument`: converts text file to document format.\n",
    "2. splitter `DocumentSplitter`: chunks the documents, default size = 200 words\n",
    "3. embedder `OpenAIDocumentEmbedder`: performs embeddings using the embedding model\n",
    "4. writer `DocumentWriter`: writes embeddings to vector database \n",
    "\n",
    "After initalizing the components, we provide component names to connect the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.converters.txt import TextFileToDocument\n",
    "from haystack.components.preprocessors.document_splitter import DocumentSplitter\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "from haystack.components.writers import DocumentWriter\n",
    "\n",
    "# Initialize components\n",
    "converter = TextFileToDocument()\n",
    "splitter = DocumentSplitter()\n",
    "embedder = OpenAIDocumentEmbedder()\n",
    "writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "# Create a pipeline and add components to it\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"converter\", converter)\n",
    "indexing_pipeline.add_component(\"splitter\", splitter)\n",
    "indexing_pipeline.add_component(\"embedder\", embedder)\n",
    "indexing_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "# Connect the components\n",
    "indexing_pipeline.connect(\"converter\", \"splitter\")\n",
    "indexing_pipeline.connect(\"splitter\", \"embedder\")\n",
    "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
    "indexing_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_pipeline.run({\"converter\": {\"sources\": ['data/davinci.txt']}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.filter_documents()[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Search Pipeline  \n",
    "\n",
    "**Note:** If there are multiple ways in which components can connect, ensure that the relevant outputs of one component are linked to the appropriate inputs of the next component.  \n",
    "In this case, we specifically connect the `query_embedder.embedding` output to the `retriever.query_embedding` input.  \n",
    "\n",
    "If components are not connected properly, a **PipelineConnectError** will occur. This error usually provides useful logs to help identify and fix the connection issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.embedders import OpenAITextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "query_embedder = OpenAITextEmbedder()\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "\n",
    "# Initialize the document search pipeline\n",
    "document_search = Pipeline()\n",
    "\n",
    "# Connect the components\n",
    "document_search.add_component(\"query_embedder\", query_embedder)\n",
    "document_search.add_component(\"retriever\", retriever)\n",
    "document_search.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "document_search.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How old was Davinci when he died?\"\n",
    "\n",
    "results = document_search.run({\"query_embedder\": {\"text\": question}})\n",
    "\n",
    "for i, document in enumerate(results[\"retriever\"][\"documents\"]):\n",
    "    print(\"\\n--------------\\n\")\n",
    "    print(f\"DOCUMENT {i}\")\n",
    "    print(document.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How old was Davinci when he died?\"\n",
    "\n",
    "results = document_search.run({\"query_embedder\": {\"text\": question}})\n",
    "\n",
    "for i, document in enumerate(results[\"retriever\"][\"documents\"]):\n",
    "    print(\"\\n--------------\\n\")\n",
    "    print(f\"DOCUMENT {i}\")\n",
    "    print(document.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How old was Davinci when he died?\"\n",
    "\n",
    "results = document_search.run({\"query_embedder\": {\"text\": question},\n",
    "                               \"retriever\": {\"top_k\": 3}}) # specify top_k to retrieve top 3 documents\n",
    "\n",
    "for i, document in enumerate(results[\"retriever\"][\"documents\"]):\n",
    "    print(\"\\n--------------\\n\")\n",
    "    print(f\"DOCUMENT {i}\")\n",
    "    print(document.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "- The above code shows how to run components individually and combining them into useful pipelines\n",
    "- We also built a indexing and a document search pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Customized RAG  \n",
    "\n",
    "We will first create a simple QA RAG pipeline and then extend it to include answer referencing.  \n",
    "\n",
    "Retrieval-Augmented Generation consists of the following main steps:  \n",
    "\n",
    "0. **Indexing** â€“ Cleaning, splitting, and storing documents in a vector database  \n",
    "1. **Retrieval** â€“ Receiving a query and retrieving the most relevant documents from the database  \n",
    "2. **Augmentation** â€“ Passing the retrieved context along with an augmentation prompt  \n",
    "3. **Generation** â€“ The LLM generates the final answer based on the prompt  \n",
    "\n",
    "Retrieval can be performed in different ways, such as:  \n",
    "- Semantic search  \n",
    "- Keyword-based search  \n",
    "- Retrieval with reranking  \n",
    "- Retrieval from external APIs (e.g., web-based or other sources)  \n",
    "\n",
    "##### **Indexing pipeline:**\n",
    "<center>\n",
    "  <img src=\"images/indexing_pipeline.png\"/>\n",
    "</center>  \n",
    "\n",
    "##### **Web Indexing pipeline:**\n",
    "<center>\n",
    "  <img src=\"images/web_indexing_pipeline.png\"/>\n",
    "</center>  \n",
    "\n",
    "##### **Semantic Search pipeline:**\n",
    "<center>\n",
    "  <img src=\"images/ss_rag_pipeline.png\"/>\n",
    "</center>  \n",
    "\n",
    "##### **Keyword Search pipeline:**\n",
    "<center>\n",
    "  <img src=\"images/key_rag_pipeline.png\">\n",
    "</center>  \n",
    "\n",
    "##### **Retrieval + Ranking pipeline:**\n",
    "<center>\n",
    "  <img src=\"images/rr_pipeline.png\">\n",
    "</center>  \n",
    "\n",
    "##### **API Retrieval pipeline:**\n",
    "<center>\n",
    "  <img src=\"images/api_pipeline.png\">\n",
    "</center>  \n",
    "\n",
    "The below code uses Cohere Document and Text embedders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from helper import load_env\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.utils.auth import Secret\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.converters import HTMLToDocument\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "from haystack_integrations.components.embedders.cohere import CohereDocumentEmbedder, CohereTextEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using documents, we will source data directly from website URLs. Since these websites do not contain much data, we can skip the document-splitting step. \n",
    "- The `LinkContentFetcher` component fetches the content of the URL  \n",
    "- The `HTMLToDocument` component converts the content into a format that Haystack can process  \n",
    "\n",
    "We then use the Cohere `embed-english-v3.0` model for embedding and store the embeddings in an in-memory vector database.  \n",
    "\n",
    "Finally, we connect the components and run the pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "fetcher = LinkContentFetcher()\n",
    "converter = HTMLToDocument()\n",
    "embedder = CohereDocumentEmbedder(model=\"embed-english-v3.0\", api_base_url=os.getenv(\"CO_API_URL\"))\n",
    "writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "indexing = Pipeline()\n",
    "indexing.add_component(\"fetcher\", fetcher)\n",
    "indexing.add_component(\"converter\", converter)\n",
    "indexing.add_component(\"embedder\", embedder)\n",
    "indexing.add_component(\"writer\", writer)\n",
    "\n",
    "indexing.connect(\"fetcher.streams\", \"converter.sources\")\n",
    "indexing.connect(\"converter\", \"embedder\")\n",
    "indexing.connect(\"embedder\", \"writer\")\n",
    "indexing.connect(\"converter\", \"writer\")\n",
    "indexing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing.run(\n",
    "    {\n",
    "        \"fetcher\": {\n",
    "            \"urls\": [\n",
    "                \"https://haystack.deepset.ai/integrations/cohere\",\n",
    "                \"https://haystack.deepset.ai/integrations/anthropic\",\n",
    "                \"https://haystack.deepset.ai/integrations/jina\",\n",
    "                \"https://haystack.deepset.ai/integrations/nvidia\",\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.filter_documents()[0] # this document comes with metadata - URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's start building a RAG pipeline.  \n",
    "\n",
    "Haystack uses **Jinja** for prompt templating. Jinja is a templating language that provides advanced capabilities such as looping within prompts. \n",
    " \n",
    "It also allows inserting specific text components into the prompt and supports features such as conditional statements (`if`), loops (`for`), truncation, lowercase conversion, and more.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the question based on the provided context.\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "   {{ doc.content }} \n",
    "{% endfor %}\n",
    "Question: {{ query }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "\n",
    "query_embedder = CohereTextEmbedder(model=\"embed-english-v3.0\", api_base_url=os.getenv(\"CO_API_URL\"))\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "prompt_builder = PromptBuilder(template=prompt)\n",
    "generator = OpenAIGenerator()\n",
    "\n",
    "rag = Pipeline()\n",
    "rag.add_component(\"query_embedder\", query_embedder)\n",
    "rag.add_component(\"retriever\", retriever)\n",
    "rag.add_component(\"prompt\", prompt_builder)\n",
    "rag.add_component(\"generator\", generator)\n",
    "\n",
    "rag.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag.connect(\"retriever.documents\", \"prompt.documents\")\n",
    "rag.connect(\"prompt\", \"generator\")\n",
    "\n",
    "rag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I use Cohere with Haystack?\"\n",
    "\n",
    "result = rag.run(\n",
    "    {\n",
    "        \"query_embedder\": {\"text\": question},\n",
    "        \"retriever\": {\"top_k\": 1},\n",
    "        \"prompt\": {\"query\": question},\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result[\"generator\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can customize the RAG pipeline:  \n",
    "\n",
    "1. Modify the prompt to answer questions in a specific language (French)  \n",
    "2. Add the URL in the prompt to generate references  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You will be provided some context, followed by the URL that this context comes from.\n",
    "Answer the question based on the context, and reference the URL from which your answer is generated.\n",
    "Your answer should be in {{ language }}.\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "   {{ doc.content }} \n",
    "   URL: {{ doc.meta['url']}}\n",
    "{% endfor %}\n",
    "Question: {{ query }}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedder = CohereTextEmbedder(model=\"embed-english-v3.0\", api_base_url=os.getenv(\"CO_API_URL\"))\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "prompt_builder = PromptBuilder(template=prompt)\n",
    "generator = OpenAIGenerator(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "rag = Pipeline()\n",
    "rag.add_component(\"query_embedder\", query_embedder)\n",
    "rag.add_component(\"retriever\", retriever)\n",
    "rag.add_component(\"prompt\", prompt_builder)\n",
    "rag.add_component(\"generator\", generator)\n",
    "\n",
    "rag.connect(\"query_embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag.connect(\"retriever.documents\", \"prompt.documents\")\n",
    "rag.connect(\"prompt\", \"generator\")\n",
    "rag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can I use Cohere with Haystack?\"\n",
    "\n",
    "result = rag.run(\n",
    "    {\n",
    "        \"query_embedder\": {\"text\": question},\n",
    "        \"retriever\": {\"top_k\": 1},\n",
    "        \"prompt\": {\"query\": question, \"language\": \"French\"},\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result[\"generator\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on:\n",
    "- LinkContentFetcher: <https://docs.haystack.deepset.ai/docs/linkcontentfetcher?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- HTMLToDocument: <https://docs.haystack.deepset.ai/docs/htmltodocument?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- CohereDocumentEmbedder: <https://docs.haystack.deepset.ai/docs/coheredocumentembedder?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- InMemoryDocumentStore: <https://docs.haystack.deepset.ai/docs/inmemorydocumentstore?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- Embedders: <https://docs.haystack.deepset.ai/docs/embedders?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- Generators: <https://docs.haystack.deepset.ai/docs/generators?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- SentenceTransformers: <https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- PromptBuilders: <https://docs.haystack.deepset.ai/docs/promptbuilder?utm_campaign=developer-relations&utm_source=dlai>\n",
    "\n",
    "Summary  :\n",
    "- We customized Haystack components to create a RAG pipeline  \n",
    "- We customized the behavior of the RAG pipeline  \n",
    "\n",
    "---\n",
    "\n",
    "# Custom Components â€“ News Summarizer  \n",
    "\n",
    "Let's create a custom component to fetch the top Hacker News posts and then summarize them.  \n",
    "\n",
    "As a refresher, some ready-made components in Haystack include: Embedders, Retrievers, Generators, Preprocessors, Rankers, Routers, etc.  \n",
    "\n",
    "Each component can accept a certain number of inputs and produce a certain number of outputs.  \n",
    "\n",
    "To create a custom component, the following requirements must be met:  \n",
    "\n",
    "1. Define a class with a `@component` decorator  \n",
    "2. The class must have a `run()` method with a `@component.output_types` decorator, specifying the componentâ€™s output types  \n",
    "3. The `run()` method should return a dictionary  \n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component\n",
    "from typing import List\n",
    "\n",
    "@component\n",
    "class Translator:\n",
    "\n",
    "    @component.output_types(documents=List[Document])\n",
    "    def run(self, \n",
    "            from_lang: str = 'en',\n",
    "            to_lang: str = 'fr',\n",
    "            documents: List[Document] = []):\n",
    "        \n",
    "        translated_docs = []\n",
    "        # translate each document from from_lang to to_lang and add them to translated_docs[]\n",
    "        return {\"documents\": translated_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we build 2 separate pipelines:\n",
    "1. Dialogue Builder - We use a custom greeter component to start with a greeting for the dialogue\n",
    "2. Hacker News Summarizer - Provides summaries of top-k trending posts on Hacker News\n",
    "\n",
    "<center>\n",
    "  <img src=\"images/hacker_news.png\"/>\n",
    "</center>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from helper import load_env\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from haystack import Document, Pipeline, component\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators.openai import OpenAIGenerator\n",
    "from haystack.components.fetchers import LinkContentFetcher\n",
    "from haystack.components.converters import HTMLToDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class Greeter:\n",
    "\n",
    "    @component.output_types(greeting=str)\n",
    "    def run(self, user_name: str):\n",
    "        return {\"greeting\": f\"Hello {user_name}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeter = Greeter()\n",
    "\n",
    "greeter.run(user_name=\"Tuana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeter = Greeter()\n",
    "template = \"\"\" You will be given the beginning of a dialogue. \n",
    "Create a short play script using this as the start of the play.\n",
    "Start of dialogue: {{ dialogue }}\n",
    "Full script: \n",
    "\"\"\"\n",
    "prompt = PromptBuilder(template=template)\n",
    "llm = OpenAIGenerator()\n",
    "\n",
    "dialogue_builder = Pipeline()\n",
    "dialogue_builder.add_component(\"greeter\", greeter)\n",
    "dialogue_builder.add_component(\"prompt\", prompt)\n",
    "dialogue_builder.add_component(\"llm\", llm)\n",
    "\n",
    "dialogue_builder.connect(\"greeter.greeting\", \"prompt.dialogue\")\n",
    "dialogue_builder.connect(\"prompt\", \"llm\")\n",
    "\n",
    "dialogue_builder.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = dialogue_builder.run({\"greeter\": {\"user_name\": \"Tuana\"}})\n",
    "dialogue\n",
    "print(dialogue[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Hacker News Summarizer\n",
    "\n",
    "We will build a component called `HackerNewsFetcher` and then create the fetcher pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this request fetches the top story post on Hacker News\n",
    "trending_list = requests.get(\n",
    "        url=\"https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\"\n",
    "    )\n",
    "post = requests.get(\n",
    "    url=f\"https://hacker-news.firebaseio.com/v0/item/{trending_list.json()[0]}.json?print=pretty\"\n",
    ")\n",
    "\n",
    "# View top story post\n",
    "print(post.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class HackernewsNewestFetcher:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        fetcher = LinkContentFetcher()\n",
    "        converter = HTMLToDocument()\n",
    "\n",
    "        html_conversion_pipeline = Pipeline()\n",
    "        html_conversion_pipeline.add_component(\"fetcher\", fetcher)\n",
    "        html_conversion_pipeline.add_component(\"converter\", converter)\n",
    "\n",
    "        html_conversion_pipeline.connect(\"fetcher\", \"converter\")\n",
    "        self.html_pipeline = html_conversion_pipeline # We add a HTML conversion pipeline to convert HTML to Document\n",
    "        \n",
    "    @component.output_types(articles=List[Document])\n",
    "    def run(self, top_k: int):\n",
    "        \n",
    "        articles = []\n",
    "        \n",
    "        trending_list = requests.get(\n",
    "            url=\"https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\"\n",
    "        )\n",
    "        \n",
    "        # loop thru the top_k posts and fetch their content - URL or text\n",
    "        for id in trending_list.json()[0:top_k]:\n",
    "            post = requests.get(\n",
    "                url=f\"https://hacker-news.firebaseio.com/v0/item/{id}.json?print=pretty\"\n",
    "            )\n",
    "            \n",
    "            if \"url\" in post.json():\n",
    "                try:\n",
    "                    article = self.html_pipeline.run(\n",
    "                        {\"fetcher\": {\"urls\": [post.json()[\"url\"]]}}\n",
    "                    )\n",
    "                    articles.append(article[\"converter\"][\"documents\"][0])\n",
    "                except:\n",
    "                    print(f\"Can't download {post}, skipped\")\n",
    "                    \n",
    "            elif \"text\" in post.json():\n",
    "                try:\n",
    "                    articles.append(Document(content=post.json()[\"text\"], meta= {\"title\": post.json()[\"title\"]}))\n",
    "                except:\n",
    "                    print(f\"Can't download {post}, skipped\")   \n",
    "        return {\"articles\": articles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = HackernewsNewestFetcher()\n",
    "results = fetcher.run(top_k=3)\n",
    "\n",
    "print(results['articles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a summarizer pipeline. We first create a prompt template and then connect the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"  \n",
    "You will be provided a few of the top posts in HackerNews.  \n",
    "For each post, provide a brief summary if possible.\n",
    "  \n",
    "Posts:  \n",
    "{% for article in articles %}\n",
    "  Post:\\n\n",
    "  {{ article.content}}\n",
    "{% endfor %}  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_builder = PromptBuilder(template=prompt_template)\n",
    "fetcher = HackernewsNewestFetcher()\n",
    "llm = OpenAIGenerator()\n",
    "\n",
    "summarizer_pipeline = Pipeline()\n",
    "summarizer_pipeline.add_component(\"fetcher\", fetcher)\n",
    "summarizer_pipeline.add_component(\"prompt\", prompt_builder)\n",
    "summarizer_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "summarizer_pipeline.connect(\"fetcher.articles\", \"prompt.articles\")\n",
    "summarizer_pipeline.connect(\"prompt\", \"llm\")\n",
    "\n",
    "summarizer_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = summarizer_pipeline.run({\"fetcher\": {\"top_k\": 3}})\n",
    "print(summaries)\n",
    "print(summaries[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's customize the prompt to include URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"  \n",
    "You will be provided a few of the top posts in HackerNews, followed by their URL.  \n",
    "For each post, provide a brief summary followed by the URL the full post can be found at.  \n",
    "  \n",
    "Posts:  \n",
    "{% for article in articles %}  \n",
    "  {{ article.content }}\n",
    "  URL: {{ article.meta[\"url\"] }}\n",
    "{% endfor %}  \n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = PromptBuilder(template=prompt_template)\n",
    "fetcher = HackernewsNewestFetcher()\n",
    "llm = OpenAIGenerator()\n",
    "\n",
    "summarizer_pipeline = Pipeline()\n",
    "summarizer_pipeline.add_component(\"fetcher\", fetcher)\n",
    "summarizer_pipeline.add_component(\"prompt\", prompt_builder)\n",
    "summarizer_pipeline.add_component(\"llm\", llm)\n",
    "\n",
    "summarizer_pipeline.connect(\"fetcher.articles\", \"prompt.articles\")\n",
    "summarizer_pipeline.connect(\"prompt\", \"llm\")\n",
    "\n",
    "summaries = summarizer_pipeline.run({\"fetcher\": {\"top_k\": 2}})\n",
    "\n",
    "print(summaries[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about the Haystack integrations:\n",
    "* [deepset-ai github repo](https://github.com/deepset-ai/haystack-integrations)\n",
    "* [haystack.deepset.ai/integrations](https://haystack.deepset.ai/integrations)\n",
    "\n",
    "----\n",
    "\n",
    "# Fallbacks with Branching Pipelines  \n",
    "\n",
    "We will now create a branching pipeline that falls back to web search when necessary. To achieve this, we use a special component called `ConditionalRouter`.  \n",
    "\n",
    "This component allows us to define custom routes and trigger specific branches of the pipeline.  \n",
    "\n",
    "<center>\n",
    "  <img src=\"images/router.png\"/>\n",
    "</center>  \n",
    "\n",
    "We will build a pipeline that first evaluates whether the answer can be obtained using a RAG pipeline. If not, the pipeline will fall back to web search.  \n",
    "\n",
    "<center>\n",
    "  <img src=\"images/web_search.png\"/>\n",
    "</center>  \n",
    "\n",
    "For the web search branch, we will build a RAG pipeline in which the retrieved documents come from URLs.  \n",
    "\n",
    "These documents will then be passed to a `PromptBuilder` and a `Generator` component.  \n",
    "\n",
    "Our first branch will be named **answer**, and the fallback branch will be named **go_to_websearch**.  \n",
    "\n",
    "<center>\n",
    "  <img src=\"images/web_search_2.png\"/>\n",
    "</center>  \n",
    "\n",
    "Since we are working with very short documents, we do not need to add a document-splitting component to our pipeline.  \n",
    "\n",
    "Instead, we will write documents directly to the document store using `write_documents()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from helper import load_env\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline, Document\n",
    "from haystack.components.routers import ConditionalRouter\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.components.websearch.serper_dev import SerperDevWebSearch\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Documents into InMemoryDocumentStore\n",
    "documents = [Document(content=\"Retrievers: Retrieves relevant documents to a user query using keyword search or semantic search.\"),\n",
    "             Document(content=\"Embedders: Creates embeddings for text or documents.\"),\n",
    "             Document(content=\"Generators: Use a number of model providers to generate answers or content based on a prompt\"),\n",
    "             Document(content=\"File Converters: Converts different file types like TXT, Markdown, PDF, etc. into a Haystack Document type\")]\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "document_store.write_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our RAG pipeline. We will use a keyword based retriever because we have not embedded our documents. We thus use the `InMemoryBM25Retriever` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template = \"\"\"\n",
    "Answer the following query given the documents.\n",
    "If the answer is not contained within the documents, reply with 'no_answer'\n",
    "Query: {{query}}\n",
    "Documents:\n",
    "{% for document in documents %}\n",
    "  {{document.content}}\n",
    "{% endfor %}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = Pipeline()\n",
    "rag.add_component(\"retriever\", InMemoryBM25Retriever(document_store=document_store))\n",
    "rag.add_component(\"prompt_builder\", PromptBuilder(template=rag_prompt_template))\n",
    "rag.add_component(\"llm\", OpenAIGenerator())\n",
    "\n",
    "rag.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag.connect(\"prompt_builder\", \"llm\")\n",
    "rag.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the pipeline reacts when we ask certain questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is a retriever for?\"\n",
    "\n",
    "rag.run({\"prompt_builder\":{\"query\": query},\n",
    "         \"retriever\": {\"query\": query}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What Mistral components are there?\"\n",
    "\n",
    "rag.run({\"prompt_builder\":{\"query\": query},\n",
    "         \"retriever\": {\"query\": query}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a conditional router. This component expects a list of routes. We start by giving 2 routes.\n",
    "\n",
    "To disable case sensitivity for the **no_answer** response, we use Jinja synatx *lower*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice - routers have 4 keys: condition, output, output_name, output_type\n",
    "# for example, the first conditions checks if 'no_answer' is in the response from the LLM (case insensitive). If so, it routes to websearch.\n",
    "routes = [\n",
    "    {\n",
    "        \"condition\": \"{{'no_answer' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_websearch\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'no_answer' not in replies[0]|lower}}\",\n",
    "        \"output\": \"{{replies[0]}}\",\n",
    "        \"output_name\": \"answer\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the router, we simulate an answer (\"Geoff is my friend\") for the query \"Who is Geoff?\". Since the string \"Geoff is my friend\" is not \"**no_answer**\", the conditional branch is not executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router = ConditionalRouter(routes=routes)\n",
    "router.run(replies=['Geoff is my friend'], query=\"Who is Geoff?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router.run(replies=['No_answer'], query=\"Who is Geoff?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the router component and create the rest of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.add_component(\"router\", ConditionalRouter(routes=routes))\n",
    "rag.connect(\"llm.replies\", \"router.replies\")\n",
    "rag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What Mistral components does Haystack have?\"\n",
    "\n",
    "rag.run({\"prompt_builder\":{\"query\": query},\n",
    "         \"retriever\": {\"query\": query},\n",
    "         \"router\": {\"query\": query}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we develop the web search branch. For this, we use the component `SerperDevWebSearch`.\n",
    "\n",
    "We then connect all components to create the final RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.websearch.serper_dev import SerperDevWebSearch\n",
    "\n",
    "prompt_for_websearch = \"\"\"\n",
    "Answer the following query given the documents retrieved from the web.\n",
    "Your answer should indicate that your answer was generated from websearch.\n",
    "You can also reference the URLs that the answer was generated from\n",
    "\n",
    "Query: {{query}}\n",
    "Documents:\n",
    "{% for document in documents %}\n",
    "  {{document.content}}\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "rag_or_websearch = Pipeline()\n",
    "rag_or_websearch.add_component(\"retriever\", InMemoryBM25Retriever(document_store=document_store))\n",
    "rag_or_websearch.add_component(\"prompt_builder\", PromptBuilder(template=rag_prompt_template))\n",
    "rag_or_websearch.add_component(\"llm\", OpenAIGenerator())\n",
    "rag_or_websearch.add_component(\"router\", ConditionalRouter(routes))\n",
    "rag_or_websearch.add_component(\"websearch\", SerperDevWebSearch())\n",
    "rag_or_websearch.add_component(\"prompt_builder_for_websearch\", PromptBuilder(template=prompt_for_websearch))\n",
    "rag_or_websearch.add_component(\"llm_for_websearch\",  OpenAIGenerator())\n",
    "\n",
    "rag_or_websearch.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_or_websearch.connect(\"prompt_builder\", \"llm\")\n",
    "rag_or_websearch.connect(\"llm.replies\", \"router.replies\")\n",
    "rag_or_websearch.connect(\"router.go_to_websearch\", \"websearch.query\")\n",
    "rag_or_websearch.connect(\"router.go_to_websearch\", \"prompt_builder_for_websearch.query\")\n",
    "rag_or_websearch.connect(\"websearch.documents\", \"prompt_builder_for_websearch.documents\")\n",
    "rag_or_websearch.connect(\"prompt_builder_for_websearch\", \"llm_for_websearch\")\n",
    "\n",
    "rag_or_websearch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"What is a retriever for?\"\n",
    "\n",
    "rag_or_websearch.run({\"prompt_builder\":{\"query\": query},\n",
    "                      \"retriever\": {\"query\": query},\n",
    "                      \"router\": {\"query\": query}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What Mistral components does Haystack have?\"\n",
    "\n",
    "rag_or_websearch.run({\"prompt_builder\":{\"query\": query},\n",
    "                      \"retriever\": {\"query\": query},\n",
    "                      \"router\": {\"query\": query}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on ConditionalRouters: <https://docs.haystack.deepset.ai/docs/conditionalrouter?utm_campaign=developer-relations&utm_source=dlai>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Self-Reflecting Agents with Loop  \n",
    "\n",
    "Let's build our first agentic pipeline â€” a self-reflecting agent capable of looping.  \n",
    "\n",
    "In this setup, an LLM will generate a set of named entities and then reflect on its own output to improve the response.  \n",
    "\n",
    "**Self-reflection consists of the following:**  \n",
    "- A mechanism for self-assessment and correction  \n",
    "- Refining outputs to provide more reliable and accurate information  \n",
    "\n",
    "This allows the LLM to provide feedback to itself for future iterations and to take actions based on predefined criteria.  \n",
    "\n",
    "<center>\n",
    "  <img src=\"images/self_reflection.png\"/>\n",
    "</center>  \n",
    "\n",
    "In this lab, we will build a simple self-reflecting agent that asks the LLM to extract entities from text.  \n",
    "\n",
    "The agent will then critique its own results, improving incorrect or incomplete entities in a loop, or return **Done** once the output is satisfactory.  \n",
    "\n",
    "We will create a custom component called `EntityValidator`, which will evaluate the generated answer.  \n",
    "\n",
    "The extracted data will be returned in JSON format, adhering to a schema. If validation fails, the pipeline will loop back for correction.  \n",
    "\n",
    "<center>\n",
    "  <img src=\"images/looping.png\"/>\n",
    "</center>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from helper import load_env\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from colorama import Fore\n",
    "from haystack import Pipeline, component\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.components.generators.openai import OpenAIGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the `EntityValidator` custom component - it either replies DONE (replaced with empty string) and outputs entities, or it outputs entities to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class EntitiesValidator:\n",
    "\n",
    "    @component.output_types(entities_to_validate=str, entities=str)\n",
    "    def run(self, replies: List[str]):\n",
    "        \n",
    "        if 'DONE' in replies[0]:\n",
    "            return {\"entities\":replies[0].replace('DONE', '')}\n",
    "        else:\n",
    "            print(Fore.RED + \"Reflecting on entities\\n\", replies[0])\n",
    "            return {\"entities_to_validate\": replies[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_validator = EntitiesValidator()\n",
    "entities_validator.run(replies= [\"{'name': 'Tuana'}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_validator.run(replies= [\"DONE {'name': 'Tuana'}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a special prompt template with an if-block. This prompt handles the following cases - extracting entities and validating entities to improve on its answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\"\n",
    "{% if entities_to_validate %}\n",
    "    Here was the text you were provided:\n",
    "    {{ text }}\n",
    "    Here are the entities you previously extracted: \n",
    "    {{ entities_to_validate[0] }}\n",
    "    Are these the correct entities? \n",
    "    Things to check for:\n",
    "    - Entity categories should exactly be \"Person\", \"Location\" and \"Date\"\n",
    "    - There should be no extra categories\n",
    "    - There should be no duplicate entities\n",
    "    - If there are no appropriate entities for a category, the category should have an empty list\n",
    "    If you are done say 'DONE' and return your new entities in the next line\n",
    "    If not, simply return the best entities you can come up with.\n",
    "    Entities:\n",
    "{% else %}\n",
    "    Extract entities from the following text\n",
    "    Text: {{ text }} \n",
    "    The entities should be presented as key-value pairs in a JSON object.\n",
    "    Example: \n",
    "    {\n",
    "        \"Person\": [\"value1\", \"value2\"], \n",
    "        \"Location\": [\"value3\", \"value4\"],\n",
    "        \"Date\": [\"value5\", \"value6\"]\n",
    "    }\n",
    "    If there are no possibilities for a particular category, return an empty list for this\n",
    "    category\n",
    "    Entities:\n",
    "{% endif %}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create our self-relflecting agent with max looping iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptBuilder(template=template)\n",
    "llm = OpenAIGenerator()\n",
    "entities_validator = EntitiesValidator()\n",
    "\n",
    "self_reflecting_agent = Pipeline(max_loops_allowed=10)\n",
    "\n",
    "self_reflecting_agent.add_component(\"prompt_builder\", prompt_template)\n",
    "self_reflecting_agent.add_component(\"entities_validator\", entities_validator)\n",
    "self_reflecting_agent.add_component(\"llm\", llm)\n",
    "\n",
    "self_reflecting_agent.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n",
    "self_reflecting_agent.connect(\"llm.replies\", \"entities_validator.replies\")\n",
    "self_reflecting_agent.connect(\"entities_validator.entities_to_validate\", \"prompt_builder.entities_to_validate\")\n",
    "\n",
    "self_reflecting_agent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Istanbul is the largest city in Turkey, straddling the Bosporus Strait, \n",
    "the boundary between Europe and Asia. It is considered the country's economic, \n",
    "cultural and historic capital. The city has a population of over 15 million residents, \n",
    "comprising 19% of the population of Turkey,[4] and is the most populous city in Europe \n",
    "and the world's fifteenth-largest city.\"\"\"\n",
    "\n",
    "result = self_reflecting_agent.run({\"prompt_builder\": {\"text\": text}})\n",
    "print(Fore.GREEN + result['entities_validator']['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Stefano: Hey all, let's start the all hands for June 6th 2024\n",
    "Geoff: Thanks, I'll kick it off with a request. Could we please add persistent memory to the Chroma document store.\n",
    "Stefano: Easy enough, I can add that to the feature requests. What else?\n",
    "Julain: There's a bug, some BM25 algorithms return negative scores and we filter them out from the results by default.\n",
    "Instead, we should probably check which algorithm is being used and keep results with negative scores accordingly.\n",
    "Esmail: Before we end this call, we should add a new Generator component for LlamaCpp in the next release.\n",
    "Tuana: Thanks all, I think we're done here, we can create some issues in GitHub about these.\"\"\"\n",
    "\n",
    "result = self_reflecting_agent.run({\"prompt_builder\": {\"text\": text}})\n",
    "print(Fore.GREEN + result['entities_validator']['entities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chat Agent with Function Calling\n",
    "\n",
    "We now use OpenAI's function calling capability to create a chat agent that uses Haystack pipelines as tools. This is done by components called `ChatGenerators`.\n",
    "\n",
    "We work with messages, which an come from: User, Agent, System or Function. \n",
    "\n",
    "Generators can be provided with a set of tools in the form on functions. The LLM decides when and how to call the function.\n",
    "\n",
    "A tool can be an external API, a simple function, or even a Haystack pipeline (e.g. a RAG pipeline that accesses your data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from helper import load_env\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import gradio as gr\n",
    "from typing import List\n",
    "from haystack import component, Pipeline, Document\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.generators.chat.openai import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.joiners import BranchJoiner\n",
    "from haystack_experimental.components.tools import OpenAIFunctionCaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a RAG pipeline and provide this RAG pipeline as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the questions based on the given context.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\"\"\"\n",
    "rag_pipe = Pipeline()\n",
    "rag_pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "rag_pipe.add_component(\"llm\", OpenAIGenerator())\n",
    "\n",
    "rag_pipe.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap the RAG pipeline as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_func(query: str):\n",
    "    documents = [\n",
    "        Document(content=\"My name is Jean and I live in Paris.\"),\n",
    "        Document(content=\"My name is Mark and I live in Berlin.\"),\n",
    "        Document(content=\"My name is Giorgio and I live in Rome.\"),\n",
    "        Document(content=\"My name is Marta and I live in Madrid.\"),\n",
    "        Document(content=\"My name is Harry and I live in London.\"),\n",
    "    ]\n",
    "    result = rag_pipe.run({\"prompt_builder\": {\"question\": query, \n",
    "                                              \"documents\": documents}})\n",
    "    return {\"reply\": result[\"llm\"][\"replies\"][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, let's create a weather function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_INFO = {\n",
    "    \"Berlin\": {\"weather\": \"mostly sunny\", \"temperature\": 7, \"unit\": \"celsius\"},\n",
    "    \"Paris\": {\"weather\": \"mostly cloudy\", \"temperature\": 8, \"unit\": \"celsius\"},\n",
    "    \"Rome\": {\"weather\": \"sunny\", \"temperature\": 14, \"unit\": \"celsius\"},\n",
    "    \"Madrid\": {\"weather\": \"sunny\", \"temperature\": 10, \"unit\": \"celsius\"},\n",
    "    \"London\": {\"weather\": \"cloudy\", \"temperature\": 9, \"unit\": \"celsius\"},\n",
    "}\n",
    "\n",
    "def get_current_weather(location: str):\n",
    "    if location in WEATHER_INFO:\n",
    "        return WEATHER_INFO[location]\n",
    "    else:\n",
    "        return {\"weather\": \"sunny\", \"temperature\": 70, \"unit\": \"fahrenheit\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our functions ready, we provide them as **tools** to the generator. \n",
    "\n",
    "We need to describe tools in a way that the OpenAI API expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rag_pipeline_func\",\n",
    "            \"description\": \"Get information about where people live\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"The city\"}\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of tools, we can provide these tools to the `OpenAIChatGenerator` - this component is not only able to work with model names but also accepts tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\", generation_kwargs={'tools': tools})\n",
    "replies = chat_generator.run(messages=[ChatMessage.from_user(\"Where does Mark live?\")])\n",
    "print(replies['replies'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call the function using the `OpenAIFunctionCaller` component - this runs the function and adds its response to the message queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_caller = OpenAIFunctionCaller(available_functions={\"rag_pipeline_func\": rag_pipeline_func, \n",
    "                                                            \"get_current_weather\": get_current_weather})\n",
    "\n",
    "results = function_caller.run(messages=replies['replies'])\n",
    "pprint.pprint(results[\"function_replies\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a chat agent that can access tools - we will create a `BranchJoiner` that connects mesages from multiple tools in one, either from the user or from the function calling tool. The message queue stores all these messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_collector = BranchJoiner(List[ChatMessage])\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\", generation_kwargs={'tools': tools})\n",
    "function_caller = OpenAIFunctionCaller(available_functions={\"rag_pipeline_func\": rag_pipeline_func, \n",
    "                                                            \"get_current_weather\": get_current_weather})\n",
    "\n",
    "# Create a chat agent that can access tools\n",
    "chat_agent = Pipeline()\n",
    "chat_agent.add_component(\"message_collector\", message_collector)\n",
    "chat_agent.add_component(\"generator\", chat_generator)\n",
    "chat_agent.add_component(\"function_caller\", function_caller)\n",
    "\n",
    "# Connect the components\n",
    "chat_agent.connect(\"message_collector\", \"generator.messages\")\n",
    "chat_agent.connect(\"generator\", \"function_caller\")\n",
    "chat_agent.connect(\"function_caller.function_replies\", \"message_collector\") # generate a human readable message from function response\n",
    "chat_agent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Message\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"\"\"If needed, break down the user's question into simpler questions and follow-up questions that you can use with your tools.\n",
    "        Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\"\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Start a chat loop\n",
    "while True:\n",
    "    user_input = input(\"INFO: Type 'exit' or 'quit' to stop\\n\")\n",
    "    if user_input.lower() == \"exit\" or user_input.lower() == \"quit\":\n",
    "        break\n",
    "    messages.append(ChatMessage.from_user(user_input)) # append to message queue\n",
    "    response = chat_agent.run({\"message_collector\": {\"value\": messages}})\n",
    "    messages.extend(response['function_caller']['assistant_replies']) # append to message queue\n",
    "    print(response['function_caller']['assistant_replies'][0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide this same exact application as a **Gradio** app. The `chat()` function below is used by Gradio to create a chat interface. Additionally, we also provide examples of what we can ask the chat agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        ChatMessage.from_system(\n",
    "            \"\"\"If needed, break down the user's question to simpler questions and follow-up questions that you can use with your tools.\n",
    "            Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\"\"\n",
    "        )\n",
    "    ]\n",
    "def chat(message, history): \n",
    "    messages.append(ChatMessage.from_user(message))\n",
    "    response = chat_agent.run({\"message_collector\": {\"value\": messages}})\n",
    "    messages.extend(response['function_caller']['assistant_replies'])\n",
    "    return response['function_caller']['assistant_replies'][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"Can you tell me where Giorgio lives?\",\n",
    "        \"What's the weather like in Madrid?\",\n",
    "        \"Who lives in London?\",\n",
    "        \"What's the weather like where Mark lives?\",\n",
    "    ],\n",
    "    title=\"Ask me about weather or where people live!\",\n",
    ")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on:\n",
    "- OpenAIChatGenerator: <https://docs.haystack.deepset.ai/docs/openaichatgenerator?utm_campaign=developer-relations&utm_source=dlai>\n",
    "- Gradio: <https://huggingface.co/gradio>\n",
    "\n",
    "---  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
